{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4dc401b",
   "metadata": {},
   "source": [
    "Word Embedding chapter 2.1 assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a88fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus/the-verdict.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "print(\"Total number of characters in the file:\", len(raw_text))\n",
    "print(raw_text[:100])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10513aa4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "text='hello, world! a;lkjf i know htat if happens'\n",
    "result=re.split(r'(\\s)',text)\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "text='hello, world! a;lkjf i know htat if happens'\n",
    "result2=re.split(r'([\\s,;])',text)\n",
    "print(result2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef9ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read the text from the file\n",
    "with open('corpus/the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    verdict_text = f.read()\n",
    "\n",
    "# Use regex to split so that every word, whitespace, and special character is a separate token\n",
    "# This will match words, whitespace, or any single non-whitespace, non-word character\n",
    "tokens = re.findall(r'\\w+|\\s+|[^\\w\\s]', verdict_text)\n",
    "\n",
    "# Print the first 50 tokens as a check\n",
    "print(tokens[:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72208955",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_whitespace = [token for token in tokens if not token.isspace()]\n",
    "print(tokens_no_whitespace[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34598981",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_no_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=sorted(set(tokens_no_whitespace))\n",
    "vocab_size=len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481bd46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: idx for idx, word in enumerate(all_words)}\n",
    "print(list(word_to_index.items())[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.idx_to_str = {idx: word for word, idx in self.str_to_idx.items()}\n",
    "        self.unk_token = \"<unk>\" if \"<unk>\" in self.str_to_idx else None\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        words = preprocessed_text.split()\n",
    "        if self.unk_token is not None:\n",
    "            return [self.str_to_idx[word] if word in self.str_to_idx else self.str_to_idx[self.unk_token] for word in words]\n",
    "        else:\n",
    "            return [self.str_to_idx[word] for word in words]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = ' '.join([self.idx_to_str[idx] if idx in self.idx_to_str else (self.unk_token if self.unk_token is not None else '') for idx in ids])\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f500ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the SimpleTokenizerV1 class with a sample text\n",
    "# Ensure the sample text only contains words from the vocab\n",
    "sample_text = \"I had always thought Jack Gisburn rather a cheap good fellow enough so it was no great surprise to me to hear that in the height of his glory he had dropped his painting married a rich widow and established himself in a villa on the Riviera\"\n",
    "tokenizer = SimpleTokenizerV1(all_words)\n",
    "\n",
    "# Encode the sample text\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "# Decode the encoded ids\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37dfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens to the vocabulary and update the mappings\n",
    "special_tokens = ['<eot>', '<unk>']\n",
    "for token in special_tokens:\n",
    "    if token not in all_words:\n",
    "        all_words.append(token)\n",
    "\n",
    "# Update vocab_size\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size (with special tokens): {vocab_size}\")\n",
    "\n",
    "\n",
    "\n",
    "# Show the last few entries to confirm special tokens are present\n",
    "print(\"Last 5 vocab entries:\", list(word_to_index.items())[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1='hello, world! a;lkjf i know htat if happens'\n",
    "text2=\"In the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera.\"\n",
    "\n",
    "text=\"<eot>\".join([text1,text2])\n",
    "print(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=SimpleTokenizerV1(all_words)\n",
    "\n",
    "print(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Common choices:\n",
    "# - \"cl100k_base\": used by GPT-4/3.5 Turbo\n",
    "# - \"gpt2\": used by GPTâ€‘2/3 legacy\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "text = \"Hello, world! The Verdict by Edith Wharton.\"\n",
    "ids = enc.encode(text)                 # list of token ids\n",
    "print(ids, len(ids))\n",
    "print(enc.decode(ids))                 # back to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a5bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = enc.decode(ids)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbef0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus/the-verdict.txt', 'r') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "\n",
    "enc_text=enc.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_100 = enc_text[:100]\n",
    "print(sample_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size=4\n",
    "\n",
    "x=enc_text[:context_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffe7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)  #1\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):  #2\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):  #3\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):  #4\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da758490",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=GPTDatasetV1(txt=raw_text,tokenizer=enc,max_length=4,stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ef64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb9fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple embedding layer\n",
    "vocab_size =50257 # assuming enc_text contains all token ids\n",
    "embedding_dim = 256  # you can choose any dimension\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Example: get embeddings for the first 10 tokens\n",
    "input_tokens = torch.tensor(enc_text[:10])\n",
    "embeddings = embedding_layer(input_tokens)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde40c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings=embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc5e648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72034fdd",
   "metadata": {},
   "source": [
    "absolute embeddings layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d37944",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length=max_length\n",
    "pos_embedding_layer=torch.nn.Embedding(context_length,embedding_dim)\n",
    "positional_embeddings=pos_embedding_layer(torch.arange(context_length))\n",
    "print(positional_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings=positional_embeddings+token_embeddings\n",
    "\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.empty(input_embeddings.shape)\n",
    "query.shape\n",
    "attention_table_col=input_embeddings.shape[1]\n",
    "attention_table_row=input_embeddings.shape[1]\n",
    "attention_scores = torch.empty(8,attention_table_row,attention_table_col)\n",
    "attention_weights = torch.empty(8,attention_table_row,attention_table_col)\n",
    "\n",
    "\n",
    "for i,element in enumerate(input_embeddings):\n",
    "    attention_scores[i]=element@element.T\n",
    "    attention_weights[i]=torch.softmax(attention_scores[i],dim=1)\n",
    "    # print(attention_weights)\n",
    "    print(attention_weights[i].sum(dim=1))\n",
    "    print(attention_weights[i].shape)\n",
    "    # print(attention_weights.shape)\n",
    "    query[i]=attention_weights[i]@element\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fae269",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(72)\n",
    "d_in=256\n",
    "d_out=72\n",
    "W_query=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_key=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_value=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f537313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = input_embeddings[0][0]\n",
    "print(input_vector.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd5c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_query=input_vector@W_query\n",
    "input_to_key=input_vector@W_key\n",
    "input_to_value=input_vector@W_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0305992",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_to_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8add72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_to_querys=input_embeddings@W_query\n",
    "inputs_to_keys=input_embeddings@W_key\n",
    "inputs_to_values=input_embeddings@W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896522e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_to_querys.shape)\n",
    "print(inputs_to_keys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_key_attention_scores=inputs_to_querys@inputs_to_keys.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_k=inputs_to_keys.shape[-1]\n",
    "print(d_k)\n",
    "query_key_attention_scores_normalized=torch.softmax(query_key_attention_scores/d_k**0.5,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36417b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector=query_key_attention_scores_normalized@inputs_to_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea55d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self,d_in,d_out):\n",
    "        super.__init__()\n",
    "        self.W_query=torch.nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_key=torch.nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_value=torch.nn.Parameter(torch.rand(d_in,d_out))\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        queries=x@self.W_query\n",
    "        keys=x@self.W_key\n",
    "        values=x@self.W_values\n",
    "        \n",
    "        attention_scores=queries@keys.T\n",
    "        attention_weights=torch.softmax(attention_scores/keys.shape[-1]**0.5,dim=-1)\n",
    "        context_vectors=attention_weights@values\n",
    "        return context_vectors\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7516443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Just used nn.Linear for better activation. \n",
    "\n",
    "\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self,d_in,d_out,qkv_bias=False):\n",
    "        super.__init__()\n",
    "        self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "    def forward(self,X):\n",
    "        queries=x@self.W_query\n",
    "        keys=x@self.W_key\n",
    "        values=x@self.W_values\n",
    "        \n",
    "        attention_scores=queries@keys.T\n",
    "        attention_weight=torch.softmax(attention_scores/keys.shape[-1]**0.5,dim=-1)\n",
    "        context_vectors=attention_weight@values\n",
    "        return context_vectors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7be28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,qkv_bias=False):\n",
    "        super.__init__()\n",
    "        self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        queries=x@self.W_query\n",
    "        keys=x@self.W_key\n",
    "        values=x@self.W_value\n",
    "        \n",
    "        attention_scores=queries@keys.T\n",
    "        mask=torch.ones(attention_scores.shape[0],attention_scores.shape[1])\n",
    "        mask=torch.triu(mask)\n",
    "        attention_scores[mask==1]=torch.tensor(-float('inf'))\n",
    "        attention_weight=torch.softmax(attention_scores/keys.shape[-1]**0.5,dim=-1)\n",
    "        # drop out here\n",
    "        # The 'train' parameter specifies whether dropout should behave in training mode (drop values) or evaluation mode (no dropout).\n",
    "        # In PyTorch, when train=True, dropout is applied; when train=False, dropout is bypassed.\n",
    "        attention_weight = torch.dropout(attention_weight, p=0.5, train=self.training)\n",
    "        context_vectors=attention_weight@values\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970323e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,drop_out,num_heads,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([CausalAttention(d_in,d_out,context_length,drop_out,qkv_bias)for _ in range(num_heads)])\n",
    "    def forward(self,x):\n",
    "        return torch.cat([head(x) for head in self.heads],dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in the image and your code are very similar, but there are a few key differences and some mistakes in your implementation.\n",
    "# Here is a version that matches the code in the image, with comments on the differences:\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out  \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.drop_out = nn.Dropout(drop_out)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Linear projections\n",
    "        keys = self.W_key(x)      # (b, num_tokens, d_out)\n",
    "        values = self.W_value(x)  # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x) # (b, num_tokens, d_out)\n",
    "\n",
    "        # Reshape for multi-head: (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose to (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Attention score: (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        # Apply mask (causal)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores = attn_scores.masked_fill(mask_bool, -torch.inf)\n",
    "\n",
    "        # Scale and softmax\n",
    "        attn_weights = torch.softmax(attn_scores / (self.head_dim ** 0.5), dim=-1)\n",
    "        attn_weights = self.drop_out(attn_weights)\n",
    "\n",
    "        # Weighted sum\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)  # (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76277403",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_configuration = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde47cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Dummy_Scratch_gpt_model(nn.Module):\n",
    "    def __init__(self,configuration):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding=nn.Embedding(configuration[\"vocab_size\"],configuration[\"emb_dim\"])\n",
    "        self.pos_emb=nn.Embedding(configuration[\"context_length\"],configuration[\"emb_dim\"])\n",
    "        self.drop_emb=nn.Dropout(configuration[\"drop_rate\"])\n",
    "        \n",
    "        self.transformer_blocks=nn.Sequential(\n",
    "            *[Dummy_Transformer_Block(configuration) for _ in range(configuration[\"n_layers\"])]\n",
    "        )\n",
    "    \n",
    "    \n",
    "        self.final_norm=Dummay_Layer_normalization(configuration[\"emb_dim\"])\n",
    "        self.out_head=nn.Linear(configuration[\"emb_dim\"],configuration[\"vocab_size\"],bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,in_dx):\n",
    "        batch_size,seq_len=in_dx.shape\n",
    "        tok_embeddings=self.token_embedding(in_dx)\n",
    "        pos_embeddings=self.pos_emb(torch.arange(seq_len,device=in_dx.device))\n",
    "        x=tok_embeddings+pos_embeddings\n",
    "        x=self.drop_emb(x)\n",
    "        x=self.transformer_blocks(x)\n",
    "        x=self.final_norm(x)\n",
    "        logits=self.out_head(x)\n",
    "        return logits\n",
    "        \n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi, device=x.device)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class Dummy_FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim=None, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = emb_dim * 4\n",
    "        self.fc1 = nn.Linear(emb_dim, hidden_dim)\n",
    "        self.gelu = GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Dummy_SelfAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            drop_out=cfg[\"drop_rate\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mha(x)\n",
    "\n",
    "class Dummy_Transformer_Block(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.ln1 = Dummay_Layer_normalization(cfg[\"emb_dim\"])\n",
    "        self.attn = Dummy_SelfAttention(cfg)\n",
    "        self.ln2 = Dummay_Layer_normalization(cfg[\"emb_dim\"])\n",
    "        self.ff = Dummy_FeedForward(cfg[\"emb_dim\"], drop_rate=cfg[\"drop_rate\"])\n",
    "    def forward(self, x):\n",
    "        # Attention block with residual\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        # Feedforward block with residual\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Dummay_Layer_normalization(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps=eps\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean=x.mean(-1,keepdim=True)\n",
    "        var=x.var(-1,keepdim=True, unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x+self.shift\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d990859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch=[]\n",
    "str1 = \"This is the first string.\"\n",
    "str2 = \"This is the second string.\"\n",
    "\n",
    "batch.append(tokenizer.encode(str1))\n",
    "batch.append(tokenizer.encode(str2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f6d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(22)\n",
    "model = Dummy_Scratch_gpt_model(gpt_configuration)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameter count:\", total_params)\n",
    "\n",
    "# Count parameters in all transformer blocks\n",
    "transformer_params = 0\n",
    "ff_params = 0\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, Dummy_Transformer_Block):\n",
    "        # All params in this transformer block\n",
    "        transformer_params += sum(p.numel() for p in module.parameters())\n",
    "        # Params in the feedforward submodule of this block\n",
    "        ff_params += sum(p.numel() for p in module.ff.parameters())\n",
    "\n",
    "print(\"Total parameters in all transformer blocks:\", transformer_params)\n",
    "print(\"Total parameters in all feedforward networks inside transformer blocks:\", ff_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c792c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the memory usage of the model (in megabytes)\n",
    "memory_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "memory_mb = memory_bytes / (1024 * 1024)\n",
    "print(\"Total model memory (MB):\", memory_mb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_input = torch.randn(2, 5, 768)  # (batch, seq_len, emb_dim)\n",
    "transformer_block = Dummy_Transformer_Block(gpt_configuration)\n",
    "output = transformer_block(sample_input)\n",
    "print(\"Input shape:\", sample_input.shape)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32927ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "gelu = GELU()\n",
    "\n",
    "x = torch.linspace(-5, 5, 1000)\n",
    "y = gelu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x.numpy(), y.detach().numpy(), label=\"GELU\")\n",
    "plt.title(\"GELU Activation Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"GELU(x)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_time_prediction(model,input_sequence,max_token_window,context_size):\n",
    "    \n",
    "    for _ in range(max_token_window):\n",
    "        input_sequence_to_model=input_sequence[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits=model(input_sequence)\n",
    "        logits=logits[:,-1,:]\n",
    "        # probs=logits\n",
    "        logits=torch.argmax(torch.softmax(logits,dim=-1),dim=-1,keepdim=True)\n",
    "        input_sequence=torch.cat((input_sequence,logits),dim=1)\n",
    "        \n",
    "    return input_sequence\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee278c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # #1 Adds batch dimension\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c92fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "out=inference_time_prediction(model,encoded_tensor,15,gpt_configuration[\"context_length\"])\n",
    "\n",
    "print(out)\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d629a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,  # 1\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,       # 2\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = Dummy_Scratch_gpt_model(GPT_CONFIG_124M)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e226ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token(text,tokenizer):\n",
    "    encoded=tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor=torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "def token_to_text(token,tokenizer):\n",
    "    flat=token.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = inference_time_prediction(\n",
    "        model,\n",
    "        text_to_token(start_context, tokenizer),\n",
    "        10,\n",
    "        GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "print(\"Output text:\\n\", token_to_text(token_ids, tokenizer))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d8f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some text and call the model\n",
    "\n",
    "# Define a new prompt for generation\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "# Tokenize the prompt and convert to tensor\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(prompt)\n",
    "prompt_tensor = torch.tensor(encoded[:5]).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Now pass the tensor to the model\n",
    "probs = model(prompt_tensor)\n",
    "# Implement similar logic to get the probabilities of the target tokens in the generated text\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get the model's output probabilities (logits -> probabilities)\n",
    "probs_softmax = F.softmax(probs, dim=-1)\n",
    "\n",
    "# For demonstration, let's get the probability of the next token at each position in the prompt\n",
    "# The \"target\" for each position is the next token in the prompt (shifted by 1)\n",
    "# For the last token, there is no next token, so we limit to len(encoded) - 1\n",
    "targets = encoded[1:]  # next tokens\n",
    "token_indices = list(range(len(targets)))  # positions 0 to len(targets)-1\n",
    "batch_idx = 0  # batch size is 1\n",
    "\n",
    "# Gather the probabilities for the target tokens at each position\n",
    "target_probas = probs_softmax[batch_idx, token_indices, targets]\n",
    "print(\"Target token probabilities for each position:\", target_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b1f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Compute the log and then mean of the target probabilities\n",
    "log_target_probas = torch.log(target_probas)\n",
    "mean_log_target_probas = -1*log_target_probas.mean()\n",
    "print(\"Mean log target probability:\", mean_log_target_probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaea9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"probs shape:\", probs.shape)\n",
    "print(\"targets shape:\", torch.tensor(targets).shape)\n",
    "loss = torch.nn.functional.cross_entropy(probs.flatten(0,1), torch.tensor(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708dfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the raw_text and print the vocabulary size\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = tokenizer.encode(raw_text)\n",
    "print(\"Number of tokens in raw_text:\", len(tokens))\n",
    "print(\"Vocabulary size:\", tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ratio = 0.9\n",
    "split_index = int(training_ratio * len(raw_text))\n",
    "train_data = raw_text[:split_index]\n",
    "test_data = raw_text[split_index:]\n",
    "\n",
    "# Create dataloaders for train and test\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=gpt_configuration[\"context_length\"],\n",
    "    stride=gpt_configuration[\"context_length\"],\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = create_dataloader_v1(\n",
    "    test_data,\n",
    "    batch_size=2,\n",
    "    max_length=gpt_configuration[\"context_length\"],\n",
    "    stride=gpt_configuration[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290da0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in test_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch,output_batch,device,model):\n",
    "    input_batch=input_batch.to(device)\n",
    "    output_batch=output_batch.to(device)\n",
    "    probs=model(input_batch)\n",
    "    loss=torch.nn.functional.cross_entropy(probs.flatten(0,1),output_batch.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_on_loader(loader, device, model):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for input_batch, output_batch in loader:\n",
    "            loss = calc_loss_batch(input_batch, output_batch, device, model)\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "    avg_loss = total_loss / total_batches if total_batches > 0 else 0.0\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14648fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_on_loader(train_loader, device, model)\n",
    "    val_loss = calc_loss_on_loader(test_loader, device, model)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b83b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,train_loader,test_loader,device,eval_iter):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_on_loader(train_loader, device, model)\n",
    "        val_loss = calc_loss_on_loader(test_loader, device, model)\n",
    "        \n",
    "    model.train()\n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc27bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model,tokenizer,device,start_context):\n",
    "    model.eval()\n",
    "    context_size=model.pos_emb.weight.shape[0]\n",
    "    encoded=text_to_token(start_context,tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids=inference_time_prediction(model=model,input_sequence=encoded,max_token_window=50,context_size=context_size)\n",
    "    decoded_text=token_to_text(token_ids,tokenizer)\n",
    "    print(decoded_text)\n",
    "    model.train()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdece1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,optimizer,tokenizer,device,no_of_epoches,train_loader,test_loader,eval_iter,eval_frequency):\n",
    "    \n",
    "    trainlosses,testlosses,track_token_seen=[],[],[]\n",
    "    token_seen,global_step=0,-1\n",
    "    \n",
    "    for epoches in range(no_of_epoches):\n",
    "        model.train()\n",
    "        for input_batch,target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss=calc_loss_batch(input_batch,target_batch,device,model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            token_seen+=input_batch.numel()\n",
    "            global_step+=1\n",
    "            \n",
    "            if global_step % eval_frequency==0:\n",
    "            \n",
    "                train_loss, test_loss=evaluate_model(model,train_loader,test_loader,device,eval_iter)\n",
    "                trainlosses.append(train_loss)\n",
    "                testlosses.append(test_loss)\n",
    "                \n",
    "                track_token_seen.append(token_seen)\n",
    "                print(f\"Ep {epoches+1}: (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"val loss {test_loss:.3f}\"\n",
    "                )\n",
    "                generate_and_print_sample(\n",
    "                    model, tokenizer, device, start_context\n",
    "                )\n",
    "                \n",
    "    return trainlosses,testlosses,track_token_seen\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbcd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = Dummy_Scratch_gpt_model(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model(\n",
    "    model, optimizer, tokenizer, device,\n",
    "    no_of_epoches=num_epochs,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    eval_iter=5,\n",
    "    eval_frequency=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b64018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
